{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\KUNAL MEHTA\\\\Desktop\\\\Data Science Training\\\\Projects\\\\Auto-Insurance-Risk-Profiling'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"kunal1406\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"c1f8c1d6722f50e4980aec7e9eba0c1df1353ad6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClassModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    class_model_path: Path\n",
    "    all_params: dict\n",
    "    class_metric_file_name: Path\n",
    "    mlflow_uri: str\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RegModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    reg_model_path: Path\n",
    "    all_params: dict\n",
    "    reg_metric_file_name: Path\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoInsurance.constants import *\n",
    "from AutoInsurance.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_class_model_evaluation_config(self) -> ClassModelEvaluationConfig:\n",
    "        config = self.config.class_model_evaluation\n",
    "        params = self.params.GradientBoostingClassifier\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        class_model_evaluation_config = ClassModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_class_path,\n",
    "            test_data_path= config.test_data_path,\n",
    "            class_model_path = config.class_model_path,\n",
    "            all_params= params,\n",
    "            class_metric_file_name = config.class_metric_file_name,\n",
    "            mlflow_uri = \"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "        )\n",
    "\n",
    "        return class_model_evaluation_config\n",
    "    \n",
    "    def get_reg_model_evaluation_config(self) -> RegModelEvaluationConfig:\n",
    "        config = self.config.reg_model_evaluation\n",
    "        params = self.params.GradientBoostingRegressor\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        reg_model_evaluation_config = RegModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path = config.train_data_reg_path,\n",
    "            test_data_path= config.test_data_path,\n",
    "            reg_model_path = config.reg_model_path,\n",
    "            all_params= params,\n",
    "            reg_metric_file_name = config.reg_metric_file_name,\n",
    "            mlflow_uri = \"https://dagshub.com/kunal1406/Auto-Insurance-Risk-Profiling.mlflow\"\n",
    "        )\n",
    "\n",
    "        return reg_model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelEvaluation:\n",
    "    def __init__(self, config: ClassModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def perform_k_fold(self, X, y):\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=45)\n",
    "        cv_scores = []\n",
    "        pred_full = np.zeros(y.shape[0]) \n",
    "        true_full = np.zeros(y.shape[0]) \n",
    "\n",
    "        i = 1\n",
    "\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            print(f\"Fold {i} started of {kf.n_splits}\")\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            gb = GradientBoostingClassifier(learning_rate=0.1,max_depth=4,max_features=0.3,min_samples_leaf=5,n_estimators=100)\n",
    "            gb.fit(X_train, y_train)\n",
    "            pred_probs = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            pred_full[test_index] = pred_probs  \n",
    "            true_full[test_index] = y_test  \n",
    "\n",
    "            score = roc_auc_score(y_test, pred_probs)\n",
    "            print('roc_auc_score', score)\n",
    "            cv_scores.append(score)\n",
    "\n",
    "            i += 1\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(true_full, pred_full)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        print(\"optimal threshold is\", optimal_threshold)\n",
    "\n",
    "        predicted_labels = (pred_full >= optimal_threshold)\n",
    "        report = classification_report(true_full, predicted_labels, output_dict=True)\n",
    "        print(report)\n",
    "\n",
    "        return gb, cv_scores, optimal_threshold, report\n",
    "\n",
    "    def evaluate_model(self, X, y):\n",
    "        gb, cv_scores, optimal_threshold, report = self.perform_k_fold(X, y)\n",
    "        mean_score = np.mean(cv_scores)\n",
    "        std_score = np.std(cv_scores)\n",
    "        print(f\"Mean roc_auc_score: {mean_score}\")\n",
    "        print(f\"Std roc_auc_score: {std_score}\")\n",
    "        return gb, mean_score, std_score, optimal_threshold, report\n",
    "    def log_into_mlflow(self):\n",
    "\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "\n",
    "            data = pd.read_csv(self.config.train_data_path)\n",
    "            X = data.drop('claim', axis=1)\n",
    "            print(X.shape)\n",
    "            y = data['claim']\n",
    "            print(y.shape)\n",
    "            model, roc_auc_score, std_roc_auc_score, optimal_threshold, report = self.evaluate_model(X, y)\n",
    "\n",
    "            scores = {\"roc_auc_score\": roc_auc_score, \"optimal_threshold\": optimal_threshold}\n",
    "            save_json(path=Path(self.config.class_metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"roc_auc_score\", roc_auc_score)\n",
    "            mlflow.log_metric(\"std roc_auc_score\", std_roc_auc_score)\n",
    "            mlflow.log_metric(\"optimal_threshold\", optimal_threshold)\n",
    "\n",
    "            for label, metric in report.items():\n",
    "                if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                    for metric_name, metric_value in metric.items():\n",
    "                        mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"GradientBoostingClassifier\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegModelEvaluation:\n",
    "    def __init__(self, config: RegModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def perform_k_fold(self, X, y):\n",
    "        model = GradientBoostingRegressor(\n",
    "            learning_rate=0.1,\n",
    "            max_depth=4,\n",
    "            max_features=0.3,\n",
    "            min_samples_leaf=5,\n",
    "            n_estimators=100\n",
    "        )\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=45)\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "        return model, cv_scores\n",
    "    \n",
    "    def evaluate_model(self, model, X_train, y_train, X_test, y_test):\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "\n",
    "        return r2, rmse, mae, predictions\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            data = pd.read_csv(self.config.train_data_path)\n",
    "            X = data.drop('log_amount', axis=1)\n",
    "            y = data['log_amount']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
    "\n",
    "            model, cv_scores = self.perform_k_fold(X, y)\n",
    "\n",
    "            r2, rmse, mae, predictions = self.evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
    "            save_json(path=Path(self.config.reg_metric_file_name), data=scores)\n",
    "\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metric(\"mean_cv_r2_score\", np.mean(cv_scores))\n",
    "            mlflow.log_metric(\"std_cv_r2_score\", np.std(cv_scores))\n",
    "            mlflow.log_metric(\"r2_score\", r2)\n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"GradientBoostingRegressor\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-23 20:40:47,262: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-05-23 20:40:47,270: INFO: common: yaml file: params.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-23 20:40:47,276: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-05-23 20:40:47,278: INFO: common: created directory at: artifacts]\n",
      "[2024-05-23 20:40:47,280: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "(60392, 29)\n",
      "(60392,)\n",
      "Fold 1 started of 10\n",
      "roc_auc_score 0.816628237186396\n",
      "Fold 2 started of 10\n",
      "roc_auc_score 0.8244962353360803\n",
      "Fold 3 started of 10\n",
      "roc_auc_score 0.8315169661785096\n",
      "Fold 4 started of 10\n",
      "roc_auc_score 0.8342509207880726\n",
      "Fold 5 started of 10\n",
      "roc_auc_score 0.8095775223970662\n",
      "Fold 6 started of 10\n",
      "roc_auc_score 0.8217031787877036\n",
      "Fold 7 started of 10\n",
      "roc_auc_score 0.8114163268732326\n",
      "Fold 8 started of 10\n",
      "roc_auc_score 0.8243160708501975\n",
      "Fold 9 started of 10\n",
      "roc_auc_score 0.8299925481696293\n",
      "Fold 10 started of 10\n",
      "roc_auc_score 0.8146559131184683\n",
      "optimal threshold is 0.15935058948563546\n",
      "{'0.0': {'precision': 0.9355055843637814, 'recall': 0.7451054366387355, 'f1-score': 0.8295200831178018, 'support': 50362.0}, '1.0': {'precision': 0.3670118343195266, 'recall': 0.7420737786640079, 'f1-score': 0.4911250412405147, 'support': 10030.0}, 'accuracy': 0.7446019340309975, 'macro avg': {'precision': 0.651258709341654, 'recall': 0.7435896076513717, 'f1-score': 0.6603225621791582, 'support': 60392.0}, 'weighted avg': {'precision': 0.84108923264594, 'recall': 0.7446019340309975, 'f1-score': 0.7733188930590327, 'support': 60392.0}}\n",
      "Mean roc_auc_score: 0.8218553919685355\n",
      "Std roc_auc_score: 0.00815558967323507\n",
      "[2024-05-23 20:41:19,134: INFO: common: json file saved at: artifacts\\model_evaluation\\class_metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'GradientBoostingClassifier'.\n",
      "2024/05/23 20:41:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GradientBoostingClassifier, version 1\n",
      "Created version '1' of model 'GradientBoostingClassifier'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-23 20:41:35,389: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2024-05-23 20:41:40,541: INFO: common: json file saved at: artifacts\\model_evaluation\\reg_metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL MEHTA\\Desktop\\Data Science Training\\Projects\\Auto-Insurance-Risk-Profiling\\venv\\lib\\site-packages\\_distutils_hack\\__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KUNAL MEHTA\\Desktop\\Data Science Training\\Projects\\Auto-Insurance-Risk-Profiling\\venv\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'GradientBoostingRegressor'.\n",
      "2024/05/23 20:41:52 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GradientBoostingRegressor, version 1\n",
      "Created version '1' of model 'GradientBoostingRegressor'.\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "\n",
    "class_model_evaluation_config = config.get_class_model_evaluation_config()\n",
    "class_model_evaluation_config = ClassModelEvaluation(config=class_model_evaluation_config)\n",
    "class_model_evaluation_config.log_into_mlflow()\n",
    "\n",
    "reg_model_evaluation_config = config.get_reg_model_evaluation_config()\n",
    "reg_model_evaluation_config = RegModelEvaluation(config=reg_model_evaluation_config)\n",
    "reg_model_evaluation_config.log_into_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
